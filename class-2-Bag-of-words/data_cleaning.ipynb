{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4832aa82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Text              clean_text\n",
      "0              I love Python!           i love python\n",
      "1    Data-Science is amazing.  datascience is amazing\n",
      "2                  What's up?                whats up\n",
      "3  Clean THIS text, please!!!  clean this text please\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'Text': [\n",
    "        \"I love Python!\",\n",
    "        \"Data-Science is amazing.\",\n",
    "        \"What's up?\",\n",
    "        \"Clean THIS text, please!!!\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to clean text: lowercase + remove punctuation\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # convert to lowercase\n",
    "    for char in string.punctuation:\n",
    "        text = text.replace(char, '')  # remove punctuation\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "df['clean_text'] = df['Text'].apply(clean_text)\n",
    "\n",
    "# Show results\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3a1c216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'clean this text, please!!!'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text =\"Clean THIS text, please!!!\"\n",
    "text = sample_text.lower()\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44c243db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "644a63b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'clean this text please'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for char in string.punctuation:\n",
    "     text = text.replace(char, '')\n",
    "text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9392736",
   "metadata": {},
   "outputs": [],
   "source": [
    "#library that contains punctuation\n",
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb1d24c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf84d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "text = \"Hello NLP! It's exciting, isn't it?\"\n",
    "\n",
    "result = \"\"  # empty string to store cleaned text\n",
    "\n",
    "for char in text:\n",
    "    if char not in string.punctuation:\n",
    "        result += char  # add the character if it's NOT punctuation\n",
    "\n",
    "print( result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9593e913",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f2afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"This is an example sentence to demonstrate stopword removal.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Remove stopwords\n",
    "words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop:\n",
    "        words.append(token.text)\n",
    "\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "160661de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bipin/Documents/NLP-TUTORIAL/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens = tokenizer(\"I loooove NLP!!! üòç\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "118b53f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  8840,  9541, 21818, 17953,  2361,   999,   999,   999,\n",
      "           100,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb7b8f2",
   "metadata": {},
   "source": [
    "* Example Mapping:\n",
    "* Token\tID\n",
    "* [CLS]\t101\n",
    "* I\t1045\n",
    "* looo (subword)\t8840\n",
    "* ##ove\t9541\n",
    "* ##e\t21818\n",
    "* NLP\t17953\n",
    "* !\t2361\n",
    "* !\t999\n",
    "* !\t999\n",
    "* !\t999\n",
    "* üòç (unknown)\t100\n",
    "* [SEP]\t102"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e99fe5a",
   "metadata": {},
   "source": [
    "* Tokenizer = automatic basic cleaner,\n",
    "* but Preprocessing = intentional smart cleaner üéØ\n",
    "\n",
    "* üîç Here's Why Preprocessing Is Still Important:\n",
    "* üîß Preprocessing Task\tüß† Why It‚Äôs Useful Beyond the Tokenizer\n",
    "* Remove URLs / emails\tNot useful for emotion/topic models ‚Äî adds noise\n",
    "* Fix typos / slang\tloooove may become junk tokens like 8840, 9541, etc.\n",
    "* Emoji to text\tüòç ‚Üí [UNK]; better to convert to \"happy\" for emotion models\n",
    "* Remove stopwords\tFocus on keywords for classic ML or simple interpretable models\n",
    "* Stemming / Lemmatization\tGroups loved, loving, love together, useful in BOW models\n",
    "* Lowercasing (for cased models)\tMakes matching consistent, esp. in noisy text\n",
    "* Custom filtering\tRemove non-ASCII, rare tokens, domain-specific junk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4999f3a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4dad494",
   "metadata": {},
   "source": [
    "* Why Preprocessing is More Important in ML (than in Transformers):\n",
    "* Aspect\tClassic ML Models\tTransformer Models (like BERT)\n",
    "* Input Type\tRequires clean tokens or features\tAccepts raw(ish) text ‚Üí uses tokenizer\n",
    "* Vocabulary Sensitivity\tVery sensitive (no subword handling)\tRobust (handles unknowns via subwords)\n",
    "* Tokenization\tMust be done manually\tHandled by tokenizer\n",
    "* Punctuation, Case\tCan confuse the model\tOften normalized internally\n",
    "* Stopwords\tMust be removed (they add noise)\tCan be kept ‚Äî model learns their usage\n",
    "* Stemming/Lemmatization\tNeeded to reduce word variation\tOften unnecessary ‚Äî model understands context\n",
    "* Emoji/URL Handling\tMust be removed or replaced\tMight still need to be cleaned (task-based)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070aa04e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
